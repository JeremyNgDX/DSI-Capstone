{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create names for the created dataframes of engineer features\n",
    "# EXCLUDE KLSI as there seeem to be something wrong with the datetime format which cant be solved\n",
    "datasets_original = ['./data/^GSPC.csv', './data/^DJI.csv', './data/^NDX.csv', \\\n",
    "                          './data/^N225.csv','./data/^STI.csv','./data/^HSI.csv',\\\n",
    "                          './data/^BSESN.csv','./data/^SSEC.csv',\\\n",
    "                          './data/^UKX.csv','./data/^JKSE.csv']\n",
    "\n",
    "dataset_names_test = ['S&P 500', 'Dow Jones', 'Nasdaq', 'Nikkei','STI','Hang Seng',\\\n",
    "                     'Bombay', 'ShangHai','FTSE100','Jakarta']\n",
    "\n",
    "feature_engineer_names  = ['./feature_engineer/' + name + '_feature_engineered.csv' for name in dataset_names_test]\n",
    "\n",
    "feature_engineer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create name_list for df_c to allow us to call the dataframe below:\n",
    "\n",
    "dataset_names_test = ['S&P 500', 'Dow Jones', 'Nasdaq', 'Nikkei','STI','Hang Seng',\\\n",
    "                     'Bombay', 'ShangHai','FTSE100','Jakarta']\n",
    "\n",
    "crash_names  = ['./crash_threshold/' + name + '_crash.csv' for name in dataset_names_test]\n",
    "\n",
    "crash_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer = pd.read_csv('./feature_engineer/S&P 500_feature_engineered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./feature_engineer/S&P 500_feature_engineered.csv',\n",
       " './feature_engineer/Dow Jones_feature_engineered.csv',\n",
       " './feature_engineer/Nasdaq_feature_engineered.csv',\n",
       " './feature_engineer/Nikkei_feature_engineered.csv',\n",
       " './feature_engineer/STI_feature_engineered.csv',\n",
       " './feature_engineer/Hang Seng_feature_engineered.csv',\n",
       " './feature_engineer/Bombay_feature_engineered.csv',\n",
       " './feature_engineer/ShangHai_feature_engineered.csv',\n",
       " './feature_engineer/FTSE100_feature_engineered.csv',\n",
       " './feature_engineer/Jakarta_feature_engineered.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create names for the created dataframes of engineer features\n",
    "# EXCLUDE KLSI as there seeem to be something wrong with the datetime format which cant be solved\n",
    "datasets_original = ['./data/^GSPC.csv','./data/^DJI.csv', './data/^NDX.csv', \\\n",
    "                          './data/^N225.csv','./data/^STI.csv','./data/^HSI.csv',\\\n",
    "                          './data/^BSESN.csv','./data/^SSEC.csv',\\\n",
    "                          './data/^UKX.csv','./data/^JKSE.csv']\n",
    "\n",
    "dataset_names_test = ['S&P 500','Dow Jones', 'Nasdaq', 'Nikkei','STI','Hang Seng',\\\n",
    "                     'Bombay', 'ShangHai','FTSE100','Jakarta']\n",
    "\n",
    "feature_engineer_names  = ['./feature_engineer/' + name + '_feature_engineered.csv' for name in dataset_names_test]\n",
    "\n",
    "feature_engineer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./crash_threshold/S&P 500_crash.csv',\n",
       " './crash_threshold/Dow Jones_crash.csv',\n",
       " './crash_threshold/Nasdaq_crash.csv',\n",
       " './crash_threshold/Nikkei_crash.csv',\n",
       " './crash_threshold/STI_crash.csv',\n",
       " './crash_threshold/Hang Seng_crash.csv',\n",
       " './crash_threshold/Bombay_crash.csv',\n",
       " './crash_threshold/ShangHai_crash.csv',\n",
       " './crash_threshold/FTSE100_crash.csv',\n",
       " './crash_threshold/Jakarta_crash.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create name_list for df_c to allow us to call the dataframe below:\n",
    "\n",
    "dataset_names_test = ['S&P 500','Dow Jones', 'Nasdaq', 'Nikkei','STI','Hang Seng',\\\n",
    "                     'Bombay', 'ShangHai','FTSE100','Jakarta']\n",
    "\n",
    "crash_names  = ['./crash_threshold/' + name + '_crash.csv' for name in dataset_names_test]\n",
    "\n",
    "crash_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in feature_engineer_names:\n",
    "    df = pd.read_csv('./feature_engineer/Nasdaq_feature_engineered.csv', index_col = 'Date')\n",
    "    df.index = pd.to_datetime(df.index, format='%Y/%m/%d')   \n",
    "    df['%change'] = df['%change'] / abs(df['%change']).mean()\n",
    "    df['volatility'] = df['volatility'] / abs(df['volatility']).mean()\n",
    "    xy = {}\n",
    "    for date in df.index[252:-126]: # <--subtract 126 days in the end\n",
    "        xy[date] = list([df['%change'][(date-timedelta(5)):date].mean()])\n",
    "        xy[date].append(df['%change'][(date-timedelta(10)):(date-timedelta(5))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(15)):(date-timedelta(10))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(21)):(date-timedelta(15))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(42)):(date-timedelta(21))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(63)):(date-timedelta(42))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(126)):(date-timedelta(63))].mean())\n",
    "        xy[date].append(df['%change'][(date-timedelta(252)):(date-timedelta(126))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(5)):date].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(10)):(date-timedelta(5))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(15)):(date-timedelta(10))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(21)):(date-timedelta(15))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(42)):(date-timedelta(21))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(63)):(date-timedelta(42))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(126)):(date-timedelta(63))].mean())\n",
    "        xy[date].append(df['volatility'][(date-timedelta(252)):(date-timedelta(126))].mean())\n",
    "\n",
    "    df_xy = pd.DataFrame.from_dict(xy, orient='index').dropna()\n",
    "    \n",
    "\n",
    "month_1 = {}\n",
    "month_2 = {}\n",
    "month_4 = {}\n",
    "month_6 = {}\n",
    "\n",
    "feature_engineer = pd.read_csv('./feature_engineer/Nasdaq_feature_engineered.csv')\n",
    "\n",
    "\n",
    "e = pd.read_csv('./crash_threshold/Nasdaq_crash.csv')\n",
    "\n",
    "for date in feature_engineer['Date'][252:-126]: \n",
    "    month_1[date] = [] #create a list for each date in the dict iterate through each date\n",
    "    month_2[date] = []\n",
    "    month_4[date] = []\n",
    "    month_6[date] = []\n",
    "    date1 = datetime.strptime(date,'%Y-%m-%d')\n",
    "    for st, end in zip(e['crash_st'], e['crash_end']):\n",
    "        st = datetime.strptime(st[0:10],'%Y-%m-%d')\n",
    "        end = datetime.strptime(end[0:10],'%Y-%m-%d')\n",
    "        if st<=date1+timedelta(1*21)<=end: #timedelta() is the number of days\n",
    "            month_1[date].append(1)\n",
    "        if st<=date1+timedelta(2*21)<=end: #timedelta() is the number of days\n",
    "            month_2[date].append(1)\n",
    "        if st<=date1+timedelta(4*21)<=end: #timedelta() is the number of days\n",
    "            month_4[date].append(1)\n",
    "        if st<=date1+timedelta(6*21)<=end: #timedelta() is the number of days\n",
    "            month_6[date].append(1)\n",
    "\n",
    "\n",
    "df_month_1 = pd.DataFrame.from_dict(month_1, orient='index')\n",
    "df_month_2 = pd.DataFrame.from_dict(month_2, orient='index')\n",
    "df_month_4 = pd.DataFrame.from_dict(month_4, orient='index')\n",
    "df_month_6 = pd.DataFrame.from_dict(month_6, orient='index')\n",
    "total_loss = pd.concat([df_xy,df_month_1,df_month_2,df_month_4,df_month_6],axis=1)\n",
    "total_loss.to_csv('./NasDaq_modelling_data.csv')\n",
    "\n",
    "dff = pd.read_csv('./NasDaq_modelling_data.csv',index_col = 'Unnamed: 0')\n",
    "dff.index = pd.to_datetime(dff.index, format='%Y/%m/%d')   \n",
    "\n",
    "dff = dff.rename(columns={'0':'%C-5','1':'%C-10','2':'%C-15','3':'%C-20','4':'%C-40','5':'%C-63','6':'%C-126','7':'%C-252',\\\n",
    "                          '8':'V-5','9':'V-10','10':'V-15','11':'V-20','12':'V-40','13':'V-63','14':'V-126','15':'V-252',\\\n",
    "                          '0.1':'1_month','0.2':'2_month','0.3':'4_month','0.4':'6_month'})\n",
    "\n",
    "dff.to_csv('./NasDaq_FINAL_MODELLING.csv') #save again with rename. Not sure why cannot work above BEFORE saving for the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-01-16</th>\n",
       "      <td>-0.021163</td>\n",
       "      <td>0.328808</td>\n",
       "      <td>0.799549</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.061327</td>\n",
       "      <td>0.473675</td>\n",
       "      <td>0.094699</td>\n",
       "      <td>-0.154771</td>\n",
       "      <td>0.322841</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>0.354363</td>\n",
       "      <td>0.245459</td>\n",
       "      <td>0.359105</td>\n",
       "      <td>0.411642</td>\n",
       "      <td>0.493623</td>\n",
       "      <td>0.484781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-17</th>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.328808</td>\n",
       "      <td>0.593952</td>\n",
       "      <td>-0.191453</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>0.443217</td>\n",
       "      <td>0.070517</td>\n",
       "      <td>-0.135479</td>\n",
       "      <td>0.306549</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>0.350887</td>\n",
       "      <td>0.263085</td>\n",
       "      <td>0.355242</td>\n",
       "      <td>0.386048</td>\n",
       "      <td>0.500806</td>\n",
       "      <td>0.482150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-18</th>\n",
       "      <td>0.150649</td>\n",
       "      <td>0.416720</td>\n",
       "      <td>1.186288</td>\n",
       "      <td>0.462851</td>\n",
       "      <td>0.006460</td>\n",
       "      <td>0.494258</td>\n",
       "      <td>0.088584</td>\n",
       "      <td>-0.112206</td>\n",
       "      <td>0.295605</td>\n",
       "      <td>0.343984</td>\n",
       "      <td>0.350317</td>\n",
       "      <td>0.290498</td>\n",
       "      <td>0.351490</td>\n",
       "      <td>0.354324</td>\n",
       "      <td>0.504155</td>\n",
       "      <td>0.479234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-21</th>\n",
       "      <td>0.097354</td>\n",
       "      <td>-0.021163</td>\n",
       "      <td>0.328808</td>\n",
       "      <td>0.681132</td>\n",
       "      <td>0.008002</td>\n",
       "      <td>0.340903</td>\n",
       "      <td>0.059636</td>\n",
       "      <td>-0.082731</td>\n",
       "      <td>0.255690</td>\n",
       "      <td>0.322841</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>0.323934</td>\n",
       "      <td>0.355630</td>\n",
       "      <td>0.343552</td>\n",
       "      <td>0.507896</td>\n",
       "      <td>0.477054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-01-22</th>\n",
       "      <td>0.434756</td>\n",
       "      <td>0.160182</td>\n",
       "      <td>0.328808</td>\n",
       "      <td>0.593952</td>\n",
       "      <td>0.139718</td>\n",
       "      <td>0.403966</td>\n",
       "      <td>0.083428</td>\n",
       "      <td>-0.096102</td>\n",
       "      <td>0.260694</td>\n",
       "      <td>0.306549</td>\n",
       "      <td>0.343279</td>\n",
       "      <td>0.350887</td>\n",
       "      <td>0.359386</td>\n",
       "      <td>0.338232</td>\n",
       "      <td>0.511376</td>\n",
       "      <td>0.473586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5         6         7         8         9        10        11        12        13        14        15  0.1  0.2  0.3  0.4\n",
       "1963-01-16 -0.021163  0.328808  0.799549  0.274763  0.061327  0.473675  0.094699 -0.154771  0.322841  0.343279  0.354363  0.245459  0.359105  0.411642  0.493623  0.484781  NaN  NaN  NaN  NaN\n",
       "1963-01-17  0.160182  0.328808  0.593952 -0.191453  0.086368  0.443217  0.070517 -0.135479  0.306549  0.343279  0.350887  0.263085  0.355242  0.386048  0.500806  0.482150  NaN  NaN  NaN  NaN\n",
       "1963-01-18  0.150649  0.416720  1.186288  0.462851  0.006460  0.494258  0.088584 -0.112206  0.295605  0.343984  0.350317  0.290498  0.351490  0.354324  0.504155  0.479234  NaN  NaN  NaN  NaN\n",
       "1963-01-21  0.097354 -0.021163  0.328808  0.681132  0.008002  0.340903  0.059636 -0.082731  0.255690  0.322841  0.343279  0.323934  0.355630  0.343552  0.507896  0.477054  NaN  NaN  NaN  NaN\n",
       "1963-01-22  0.434756  0.160182  0.328808  0.593952  0.139718  0.403966  0.083428 -0.096102  0.260694  0.306549  0.343279  0.350887  0.359386  0.338232  0.511376  0.473586  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = pd.read_csv('./SNP_modelling_data.csv',index_col = 'Unnamed: 0')\n",
    "dff.index = pd.to_datetime(dff.index, format='%Y/%m/%d')   \n",
    "dff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = dff.rename(columns={'0':'%C-5','1':'%C-10','2':'%C-15','3':'%C-20','4':'%C-40','5':'%C-63','6':'%C-126','7':'%C-252',\\\n",
    "                          '8':'V-5','9':'V-10','10':'V-15','11':'V-20','12':'V-40','13':'V-63','14':'V-126','15':'V-252',\\\n",
    "                          '0.1':'1_month','0.2':'2_month','0.3':'4_month','0.4':'6_month'})\n",
    "\n",
    "dff.to_csv('./SNP500_FINAL_MODELLING.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./feature_engineer/S&P 500_feature_engineered.csv',\n",
       " './feature_engineer/Dow Jones_feature_engineered.csv',\n",
       " './feature_engineer/Nasdaq_feature_engineered.csv',\n",
       " './feature_engineer/Nikkei_feature_engineered.csv',\n",
       " './feature_engineer/STI_feature_engineered.csv',\n",
       " './feature_engineer/Hang Seng_feature_engineered.csv',\n",
       " './feature_engineer/Bombay_feature_engineered.csv',\n",
       " './feature_engineer/ShangHai_feature_engineered.csv',\n",
       " './feature_engineer/FTSE100_feature_engineered.csv',\n",
       " './feature_engineer/Jakarta_feature_engineered.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create names for the created dataframes of engineer features\n",
    "# EXCLUDE KLSI as there seeem to be something wrong with the datetime format which cant be solved\n",
    "datasets_original = ['./data/^GSPC.csv', './data/^DJI.csv', './data/^NDX.csv', \\\n",
    "                          './data/^N225.csv','./data/^STI.csv','./data/^HSI.csv',\\\n",
    "                          './data/^BSESN.csv','./data/^SSEC.csv',\\\n",
    "                          './data/^UKX.csv','./data/^JKSE.csv']\n",
    "\n",
    "dataset_names_test = ['S&P 500', 'Dow Jones', 'Nasdaq', 'Nikkei','STI','Hang Seng',\\\n",
    "                     'Bombay', 'ShangHai','FTSE100','Jakarta']\n",
    "\n",
    "feature_engineer_names  = ['./feature_engineer/' + name + '_feature_engineered.csv' for name in dataset_names_test]\n",
    "\n",
    "feature_engineer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING CODE FOR MULTIPLE MONTHS\n",
    "# Something wrong with looping, key in feature engineer and crash names manually one by one!\n",
    "\n",
    "feature_engineer = pd.read_csv('./feature_engineer/S&P 500_feature_engineered.csv')\n",
    "crash = pd.read_csv('./crash_threshold/S&P 500_crash.csv')\n",
    "\n",
    "month_1 = {}\n",
    "month_2 = {}\n",
    "month_4 = {}\n",
    "month_6 = {}\n",
    "\n",
    "for date in feature_engineer['Date'][252:-126]: \n",
    "    month_1[date] = [] #create a list for each date in the dict iterate through each date\n",
    "    month_2[date] = []\n",
    "    month_4[date] = []\n",
    "    month_6[date] = []\n",
    "    date1 = datetime.strptime(date,'%Y-%m-%d')\n",
    "    for st, end in zip(crash['crash_st'], crash['crash_end']):\n",
    "        st = datetime.strptime(st[0:10],'%Y-%m-%d')\n",
    "        end = datetime.strptime(end[0:10],'%Y-%m-%d')\n",
    "        if st<=date1+timedelta(1*21)<=end: #timedelta() is the number of days\n",
    "            month_1[date].append(1)\n",
    "        if st<=date1+timedelta(2*21)<=end: #timedelta() is the number of days\n",
    "            month_2[date].append(1)\n",
    "        if st<=date1+timedelta(4*21)<=end: #timedelta() is the number of days\n",
    "            month_4[date].append(1)\n",
    "        if st<=date1+timedelta(6*21)<=end: #timedelta() is the number of days\n",
    "            month_6[date].append(1)\n",
    "        \n",
    "            \n",
    "df_month_1 = pd.DataFrame.from_dict(month_1, orient='index')\n",
    "df_month_2 = pd.DataFrame.from_dict(month_2, orient='index')\n",
    "df_month_4 = pd.DataFrame.from_dict(month_4, orient='index')\n",
    "df_month_6 = pd.DataFrame.from_dict(month_6, orient='index')\n",
    "total_loss = pd.concat([df_month_1,df_month_2,df_month_4,df_month_6],axis=1)\n",
    "total_loss.to_csv('./SNP_LOSS_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating truth values for crash in 1, 2, 4 and 6 months\n",
    "\n",
    "for d in crash_names:\n",
    "    crash = pd.read_csv(d, index_col = 'Date')\n",
    "    crash.index = pd.to_datetime(crash.index, format='%Y/%m/%d')   \n",
    "\n",
    "    month_1 = {}\n",
    "    month_2 = {}\n",
    "    month_4 = {}\n",
    "    month_6 = {}\n",
    "    \n",
    "    for e in feature_engineer_names:\n",
    "        names = pd.read_csv(e, index_col = 'Date')\n",
    "        names.index = pd.to_datetime(names.index, format='%Y/%m/%d')   \n",
    "\n",
    "        for date in names['Date'][252:-126]: \n",
    "            month_1[date] = [] #create a list for each date in the dict iterate through each date\n",
    "            month_2[date] = []\n",
    "            month_4[date] = []\n",
    "            month_6[date] = []\n",
    "            date1 = datetime.strptime(date,'%Y-%m-%d')\n",
    "            for st, end in zip(crash['crash_st'], crash['crash_end']):\n",
    "                st = datetime.strptime(st[0:10],'%Y-%m-%d')\n",
    "                end = datetime.strptime(end[0:10],'%Y-%m-%d')\n",
    "                if st<=date1+timedelta(1*21)<=end: #timedelta() is the number of days\n",
    "                    month_1[date].append(1)\n",
    "                if st<=date1+timedelta(2*21)<=end: #timedelta() is the number of days\n",
    "                    month_2[date].append(1)\n",
    "                if st<=date1+timedelta(4*21)<=end: #timedelta() is the number of days\n",
    "                    month_4[date].append(1)\n",
    "                if st<=date1+timedelta(6*21)<=end: #timedelta() is the number of days\n",
    "                    month_6[date].append(1)\n",
    "\n",
    "\n",
    "        df_month_1 = pd.DataFrame.from_dict(month_1, orient='index')\n",
    "        df_month_2 = pd.DataFrame.from_dict(month_2, orient='index')\n",
    "        df_month_4 = pd.DataFrame.from_dict(month_4, orient='index')\n",
    "        df_month_6 = pd.DataFrame.from_dict(month_6, orient='index')\n",
    "        total_loss = pd.concat([df_month_1,df_month_2,df_month_4,df_month_6],axis=1)\n",
    "\n",
    "        total_loss.to_csv('./{}1.csv'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
